"""Execution engine for Gryffin - executes tasks with testing and validation."""
from __future__ import annotations

import json
import subprocess
from dataclasses import dataclass
from pathlib import Path
from typing import Any

from .llm import generate_json
from .debugger import analyze_and_fix_test_failure, get_file_tree_with_contents, DebugFix
import platform
from datetime import datetime

MAX_AUTO_RETRY_ATTEMPTS = 15  # Ralph Wiggum mode: "I'm helping!" until it works


def log_user_interaction(target_dir: Path, context: str, choice: str, instructions: str = "") -> None:
    """
    Log user interactions to a conversation history file.

    Args:
        target_dir: Project directory
        context: What was being done when user provided input
        choice: User's choice/response
        instructions: Additional instructions provided by user
    """
    log_file = target_dir / "gryffin_conversation.log"

    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_entry = f"""
[{timestamp}]
Context: {context}
User Choice: {choice}
"""

    if instructions:
        log_entry += f"Additional Instructions: {instructions}\n"

    log_entry += "-" * 80 + "\n"

    try:
        with log_file.open("a", encoding="utf-8") as f:
            f.write(log_entry)
    except Exception as e:
        # Don't fail execution if logging fails
        print(f"‚ö†Ô∏è  Failed to log interaction: {e}")


def generate_readme(
    architecture: dict[str, Any],
    target_dir: Path,
    file_tree_snapshot: dict[str, Any]
) -> str:
    """
    Generate a comprehensive README.md for the project.

    This README serves as context for all agents (code generation, debugging, etc.)
    """
    print("\nüìù Generating README.md...")

    # Get system information
    os_info = platform.system()
    os_version = platform.release()
    python_version = platform.python_version()

    # Detect tech versions
    tech_versions = {}

    # Check for Node.js
    try:
        result = subprocess.run(
            "node --version",
            shell=True,
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            tech_versions["Node.js"] = result.stdout.strip()
    except:
        pass

    # Check for Django
    try:
        result = subprocess.run(
            "python -m django --version",
            shell=True,
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            tech_versions["Django"] = result.stdout.strip()
    except:
        pass

    # Generate file tree representation
    file_tree_str = "\n".join([f"‚îú‚îÄ‚îÄ {f}" for f in file_tree_snapshot.get("files", [])[:30]])

    # Build README content
    readme_content = f"""# {architecture.get('app_name', 'Project')}

> Generated by [Gryffin](https://github.com/gryffin) - AI-powered development tool

## Overview

{architecture.get('overview', 'No overview available.')}

## What This App IS

"""

    # Add components as "what this app IS"
    components = architecture.get('components', {})
    if isinstance(components, dict):
        for name, details in components.items():
            if isinstance(details, dict):
                readme_content += f"- **{name.title()}**: {details.get('functionality', 'Component functionality')}\n"
            else:
                readme_content += f"- **{name.title()}**: {details}\n"

    readme_content += f"""
## What This App IS NOT

"""

    # Add limitations/what it's not based on architecture
    assumptions = architecture.get('assumptions', [])
    if assumptions:
        for assumption in assumptions:
            # Convert assumptions to negative statements
            readme_content += f"- Not designed for: {assumption.replace('Users will', 'Scenarios where users will not')}\n"
    else:
        readme_content += "- Not a production-ready application (MVP/prototype stage)\n"
        readme_content += "- Not fully tested in all environments\n"

    readme_content += f"""
## File Structure

```
{architecture.get('app_name', 'project')}/
{file_tree_str}
```

## System Configuration

- **Operating System**: {os_info} {os_version}
- **Python Version**: {python_version}
"""

    # Add detected tech versions
    for tech, version in tech_versions.items():
        readme_content += f"- **{tech}**: {version}\n"

    readme_content += """
## Tech Stack

"""

    # Add tech stack details
    tech_stack = architecture.get('tech_stack', {})
    if isinstance(tech_stack, dict):
        for component, details in tech_stack.items():
            readme_content += f"\n### {component.title()}\n\n"
            if isinstance(details, dict):
                framework = details.get('framework', 'N/A')
                readme_content += f"- **Framework**: {framework}\n"

                libraries = details.get('libraries', [])
                if libraries:
                    readme_content += f"- **Libraries**:\n"
                    for lib in libraries:
                        readme_content += f"  - {lib}\n"

                version = details.get('version', '')
                if version:
                    readme_content += f"- **Version**: {version}\n"

    readme_content += """
## Data Flow

"""

    # Add data flow
    data_flow = architecture.get('data_flow', {})
    if isinstance(data_flow, dict):
        for step, description in sorted(data_flow.items()):
            readme_content += f"{step}. {description}\n"
    elif isinstance(data_flow, str):
        readme_content += f"{data_flow}\n"

    # Add risks
    risks = architecture.get('risks', [])
    if risks:
        readme_content += """
## Known Risks & Limitations

"""
        for risk in risks:
            readme_content += f"- ‚ö†Ô∏è  {risk}\n"

    # Add assumptions
    if assumptions:
        readme_content += """
## Assumptions

"""
        for assumption in assumptions:
            readme_content += f"- {assumption}\n"

    readme_content += """
## Development

This project was generated and is being developed using Gryffin, an AI-powered development tool that:
- Generates architecture and task plans
- Implements features autonomously
- Tests and debugs code automatically
- Maintains this README for context

### For AI Agents

This README provides essential context for all AI agents working on this project:
- **Architecture**: Defines the high-level structure and components
- **Tech Stack**: Specifies technologies and versions in use
- **File Structure**: Shows organization of code and resources
- **System Config**: Indicates the development environment
- **Limitations**: Clarifies what this application is NOT designed for

When implementing features or debugging, always reference this README to maintain consistency with the project's architecture and constraints.

---

*Last updated: Auto-generated at project initialization*
"""

    # Write README to file
    readme_path = target_dir / "README.md"
    readme_path.write_text(readme_content, encoding="utf-8")

    print(f"‚úì README.md created at {readme_path}")

    return readme_content


def get_user_input(
    prompt: str,
    allow_instructions: bool = True,
    target_dir: Path | None = None,
    context: str | None = None
) -> tuple[str, str]:
    """
    Get user input with optional additional instructions.

    Args:
        prompt: The prompt to show the user
        allow_instructions: Whether to allow additional instructions
        target_dir: Optional directory to log interaction to
        context: Optional context description for logging

    Returns:
        (choice, instructions) - choice is the main response, instructions are additional feedback
    """
    print(f"\n{prompt}")
    if allow_instructions:
        print("  üí° Tip: You can add instructions after your choice (e.g., 'y, but also install redis')")

    user_input = input("\n> ").strip()

    # Parse input for choice and additional instructions
    if ',' in user_input:
        parts = user_input.split(',', 1)
        choice = parts[0].strip().lower()
        instructions = parts[1].strip()
    else:
        choice = user_input.lower()
        instructions = ""

    # Log interaction if target_dir and context provided
    if target_dir and context:
        log_user_interaction(target_dir, context, choice, instructions)

    return choice, instructions


def auto_fix_error(
    error_message: str,
    context: str,
    previous_attempt: str,
    retry_count: int
) -> dict[str, Any]:
    """
    Use LLM to analyze error and suggest fix.

    Args:
        error_message: The error message/output
        context: What was being attempted
        previous_attempt: The code/command that failed
        retry_count: How many times we've tried

    Returns:
        Dict with 'solution', 'explanation', 'confidence' (high/medium/low)
    """
    print(f"\nü§ñ Analyzing error (attempt {retry_count + 1}/{MAX_AUTO_RETRY_ATTEMPTS})...")

    fix_prompt = f"""An error occurred while {context}.

Previous attempt:
```
{previous_attempt}
```

Error message:
```
{error_message}
```

Retry count: {retry_count}

IMPORTANT: Your goal is to FIX THIS ERROR AUTONOMOUSLY. Only flag needs_human=true if it's IMPOSSIBLE to fix without human intervention (e.g., requires external credentials, manual API setup, physical access).

For common errors you CAN and SHOULD fix:
- Missing files/directories ‚Üí Create them
- Missing Django/Node/Python project structure ‚Üí Initialize it with proper commands (django-admin startproject, npm init, etc.)
- Missing dependencies ‚Üí Install them
- Wrong directory ‚Üí cd to correct directory or create it
- Configuration issues ‚Üí Generate proper config
- Missing environment variables ‚Üí Create .env with placeholders

Analyze this error and provide an AUTONOMOUS fix. Return JSON with:
- solution: The corrected code/command(s) to try - can be multiple commands separated by &&
- explanation: Brief explanation of what went wrong and how your fix addresses it
- confidence: "high" if you're confident this will work, "medium" if unsure, "low" if unlikely to work
- needs_human: true ONLY if truly impossible to fix autonomously (default: false)
- human_instructions: (only if needs_human=true) detailed step-by-step instructions for the human
"""

    result = generate_json(
        "You are a senior debugging engineer who ALWAYS tries to fix errors autonomously. Only escalate to humans as an absolute last resort. Return only valid JSON with solution, explanation, confidence, needs_human (default false), and optionally human_instructions.",
        fix_prompt
    )

    if not result or not isinstance(result, dict):
        return {
            "solution": None,
            "explanation": "Could not generate auto-fix",
            "confidence": "low",
            "needs_human": True,
            "human_instructions": "Please review the error manually and fix the issue."
        }

    return result


@dataclass
class ExecutionContext:
    """Context for task execution."""
    target_dir: Path
    architecture: dict[str, Any]
    tasks: dict[str, Any]
    completed_tasks: list[str]
    file_tree_snapshot: dict[str, Any]
    readme_content: str = ""


@dataclass
class TestResult:
    """Result of a test run."""
    passed: bool
    output: str
    errors: list[str]
    test_count: int
    failed_count: int


def take_file_tree_snapshot(target_dir: Path) -> dict[str, Any]:
    """Take a snapshot of the current file tree structure."""
    print("\nüì∏ Taking file tree snapshot...")

    snapshot = {
        "files": [],
        "directories": [],
        "key_files": {}
    }

    # Key files to look for
    key_patterns = [
        "package.json",
        "requirements.txt",
        "Pipfile",
        "pyproject.toml",
        ".env",
        ".env.example",
        "Dockerfile",
        "docker-compose.yml",
        "README.md",
        "Makefile"
    ]

    for item in target_dir.rglob("*"):
        # Skip common directories to ignore
        if any(part.startswith(".") or part in ["node_modules", "__pycache__", "venv", "env", ".git"]
               for part in item.parts):
            continue

        relative_path = item.relative_to(target_dir)

        if item.is_file():
            snapshot["files"].append(str(relative_path))

            # Check if it's a key file
            if item.name in key_patterns:
                snapshot["key_files"][item.name] = str(relative_path)
        elif item.is_dir():
            snapshot["directories"].append(str(relative_path))

    print(f"‚úì Found {len(snapshot['files'])} files and {len(snapshot['directories'])} directories")
    print(f"‚úì Key files detected: {', '.join(snapshot['key_files'].keys()) or 'None'}")

    return snapshot


def detect_environment(snapshot: dict[str, Any], target_dir: Path) -> dict[str, Any]:
    """Detect the project environment and what needs to be set up."""
    print("\nüîç Detecting environment...")

    env_info = {
        "project_type": None,
        "has_env_file": False,
        "needs_setup": [],
        "detected_dependencies": []
    }

    key_files = snapshot.get("key_files", {})

    # Detect project type
    if "package.json" in key_files:
        env_info["project_type"] = "node"
        env_info["detected_dependencies"].append("npm/yarn")
        if not (target_dir / "node_modules").exists():
            env_info["needs_setup"].append("npm install")

    if "requirements.txt" in key_files or "pyproject.toml" in key_files or "Pipfile" in key_files:
        env_info["project_type"] = "python"
        env_info["detected_dependencies"].append("pip/poetry")
        if not any((target_dir / d).exists() for d in ["venv", "env", ".venv"]):
            env_info["needs_setup"].append("python virtual environment")

    # Check for .env file
    if ".env" in key_files:
        env_info["has_env_file"] = True
        print("‚úì .env file found")
    elif ".env.example" in key_files:
        env_info["needs_setup"].append("copy .env.example to .env")
        print("‚ö†Ô∏è  .env.example found but no .env file")
    else:
        env_info["needs_setup"].append("create .env file")
        print("‚ö†Ô∏è  No .env file detected")

    # Check for Docker
    if "Dockerfile" in key_files or "docker-compose.yml" in key_files:
        env_info["detected_dependencies"].append("docker")

    print(f"‚úì Project type: {env_info['project_type'] or 'unknown'}")
    if env_info["needs_setup"]:
        print(f"‚ö†Ô∏è  Setup needed: {', '.join(env_info['needs_setup'])}")

    return env_info


def setup_environment(env_info: dict[str, Any], target_dir: Path, architecture: dict[str, Any]) -> bool:
    """Set up the project environment based on detected needs."""
    print("\nüîß Setting up environment...")

    needs_setup = env_info.get("needs_setup", [])

    if not needs_setup:
        print("‚úì Environment already set up")
        return True

    # Use LLM to generate setup instructions
    setup_prompt = f"""Given this project architecture:
{json.dumps(architecture, indent=2)}

And these detected setup needs:
{', '.join(needs_setup)}

Project type: {env_info.get('project_type', 'unknown')}
Current directory: {target_dir}

Generate COMPLETE setup commands including:

1. PROJECT INITIALIZATION (if needed):
   - For Django: Create project structure with django-admin startproject
     Example: mkdir -p backend && cd backend && django-admin startproject config . && cd ..
   - For Node: npm init or create-react-app/create-next-app
   - For Flask: Create basic app structure

2. DEPENDENCIES:
   - Install required packages (pip install, npm install, etc.)

3. CONFIGURATION:
   - Create .env file with all necessary environment variables
   - Create any required config files

4. DATABASE:
   - Run migrations ONLY AFTER project structure exists
   - Set up initial database if needed

IMPORTANT:
- Commands must be in the correct order (initialize ‚Üí install ‚Üí configure ‚Üí migrate)
- For Django, ALWAYS create the project structure BEFORE running manage.py commands
- Use relative paths and proper directory navigation
- Test each command would actually work

Return JSON with key 'setup_commands' (array of shell commands to run in sequence).
"""

    result = generate_json(
        "You are a senior DevOps engineer who creates COMPLETE, working setup scripts. Always initialize project structure before running framework-specific commands. Return only valid JSON with setup_commands array.",
        setup_prompt
    )

    if result and isinstance(result, dict):
        commands = result.get("setup_commands", [])
        print(f"\nüìã Setup plan: {len(commands)} commands")
        for i, cmd in enumerate(commands, 1):
            print(f"  {i}. {cmd}")

        # Get user input with optional instructions
        choice, additional_instructions = get_user_input(
            "Proceed with setup? (y/n, or add instructions like 'y, but also install redis')",
            target_dir=target_dir,
            context="Environment setup approval"
        )

        if additional_instructions:
            print(f"\nüí° Adding to setup: {additional_instructions}")
            # Regenerate with additional instructions
            updated_prompt = f"{setup_prompt}\n\nAdditional user requirements: {additional_instructions}"
            updated_result = generate_json(
                "You are a senior DevOps engineer who creates COMPLETE, working setup scripts. Always initialize project structure before running framework-specific commands. Return only valid JSON with setup_commands array.",
                updated_prompt
            )
            if updated_result and isinstance(updated_result, dict):
                commands = updated_result.get("setup_commands", [])
                print(f"\nüìã Updated setup plan: {len(commands)} commands")
                for i, cmd in enumerate(commands, 1):
                    print(f"  {i}. {cmd}")

        if choice == "y" or choice == "yes":
            for cmd in commands:
                success = run_command_with_retry(cmd, target_dir, f"running setup command: {cmd}")
                if not success:
                    return False

            print("\n‚úÖ Environment setup complete!")
            return True

    print("‚ö†Ô∏è  Could not generate setup plan. Please set up manually.")
    return False


def run_command_with_retry(
    command: str,
    cwd: Path,
    context: str,
    max_retries: int = MAX_AUTO_RETRY_ATTEMPTS
) -> bool:
    """
    Run a command with automatic retry on failure.

    Args:
        command: Shell command to run
        cwd: Working directory
        context: Description of what this command does
        max_retries: Maximum number of retry attempts

    Returns:
        True if successful, False if all retries failed
    """
    retry_count = 0
    current_command = command
    last_error = None
    stuck_count = 0  # Track if we're stuck with the same error

    while retry_count < max_retries:
        print(f"\n‚ñ∂ Running: {current_command}")

        try:
            result = subprocess.run(
                current_command,
                shell=True,
                cwd=cwd,
                capture_output=True,
                text=True,
                timeout=300
            )

            if result.returncode == 0:
                print(f"‚úì Success")
                return True

            # Command failed - try to auto-fix
            print(f"‚úó Failed with error:")
            error_output = result.stderr or result.stdout
            print(f"  {error_output[:500]}")  # Show first 500 chars

            # Check if we're stuck with the same error
            if last_error and error_output[:300] == last_error[:300]:
                stuck_count += 1
                if stuck_count >= 3:
                    print(f"\n‚ö†Ô∏è  Same error seen {stuck_count} times - trying radically different approach...")
            else:
                stuck_count = 0
            last_error = error_output

            if retry_count < max_retries - 1:
                print(f"\nüîÑ Retrying... (attempt {retry_count + 1}/{max_retries})")

                # Get auto-fix suggestion with stuck context
                context_msg = context
                if stuck_count >= 3:
                    context_msg += f" (CRITICAL: Same error repeated {stuck_count} times - previous fixes FAILED, try a COMPLETELY DIFFERENT approach)"

                fix_result = auto_fix_error(
                    error_message=error_output,
                    context=context_msg,
                    previous_attempt=current_command,
                    retry_count=retry_count
                )

                if fix_result.get("needs_human"):
                    print(f"\n‚ö†Ô∏è  Human intervention needed:")
                    print(f"\n{fix_result.get('explanation', 'Unknown error')}")
                    print(f"\nüìã Required action:")
                    print(f"  {fix_result.get('human_instructions', 'Please review and fix manually')}")

                    choice, instructions = get_user_input(
                        "Options:\n  1. I've fixed it, retry the original command\n  2. Skip this step\n  3. Abort\n\nYour choice"
                    )

                    if choice == "1":
                        # Retry original command
                        retry_count += 1
                        continue
                    elif choice == "2":
                        print("‚è≠Ô∏è  Skipping this step...")
                        return True
                    else:
                        print("‚ùå Aborting...")
                        return False

                # Try the suggested fix
                print(f"\nüí° Fix suggestion: {fix_result.get('explanation', 'Trying alternative approach')}")
                current_command = fix_result.get("solution", current_command)

            retry_count += 1

        except subprocess.TimeoutExpired:
            print(f"‚úó Command timed out after 300 seconds")
            retry_count += 1
            if retry_count < max_retries:
                print(f"\nüîÑ Retrying... (attempt {retry_count}/{max_retries})")
        except Exception as e:
            print(f"‚úó Error: {e}")
            retry_count += 1
            if retry_count < max_retries:
                print(f"\nüîÑ Retrying... (attempt {retry_count}/{max_retries})")

    print(f"\n‚ùå Command failed after {max_retries} attempts")
    return False


def generate_task_code(
    task: dict[str, Any],
    context: ExecutionContext,
    task_index: int
) -> dict[str, Any]:
    """Generate code for a specific task using LLM."""
    print(f"\nüíª Generating code for task {task_index + 1}: {task.get('title')}...")

    prompt = f"""You are implementing this task:

Task: {task.get('title')}
Description: {task.get('description')}

## Project README (IMPORTANT - Read First!)
{context.readme_content if context.readme_content else "No README available"}

## Project Architecture
{json.dumps(context.architecture, indent=2)}

## Implementation Context
Already completed tasks: {', '.join(context.completed_tasks) or 'None'}

Current file tree:
{json.dumps(context.file_tree_snapshot, indent=2)[:3000]}

## Your Task
Generate a complete, production-ready implementation for this task.

CRITICAL RULES:
1. **Follow the README**: The project README defines what this app IS and IS NOT. Stay within those bounds.
2. **Respect the architecture**: Use the specified tech stack and follow the defined data flow.
3. **Proper file locations**: Place files in the correct directories based on the file tree structure.
4. **Django projects**: If this is a Django project, put code in proper Django apps (not in the root).
5. **Complete imports**: Include all necessary import statements.
6. **Proper test setup**: For Django, include proper pytest configuration (conftest.py, pytest.ini).
7. **Mock external services**: Mock any external APIs (OpenAI, Google, etc.) in tests.

Generate the implementation. Return JSON with:
- files: array of {{path: "relative/path/in/correct/location", content: "complete file content with all imports", action: "create" | "modify"}}
- tests: array of {{path: "tests/path", content: "complete test with mocks", type: "unit" | "integration"}}
- description: string explaining what was implemented and where files were placed
"""

    result = generate_json(
        "You are a senior software engineer who meticulously follows project architecture and README guidelines. You place files in correct locations, include all imports, mock external services in tests, and create production-ready code. Return only valid JSON with files, tests, and description.",
        prompt
    )

    if not result or not isinstance(result, dict):
        print("‚úó Failed to generate code")
        return {}

    print(f"‚úì Generated {len(result.get('files', []))} files and {len(result.get('tests', []))} tests")
    return result


def apply_code_changes(code_result: dict[str, Any], target_dir: Path) -> bool:
    """Apply the generated code changes to the file system."""
    print("\nüìù Applying code changes...")

    files = code_result.get("files", [])

    for file_info in files:
        file_path = target_dir / file_info["path"]
        action = file_info.get("action", "create")
        content = file_info.get("content", "")

        try:
            file_path.parent.mkdir(parents=True, exist_ok=True)

            if action == "create":
                if file_path.exists():
                    print(f"‚ö†Ô∏è  File already exists: {file_info['path']}")
                    choice, instructions = get_user_input(
                        f"Overwrite {file_info['path']}? (y/n, or add instructions like 'y, but keep the existing imports')",
                        allow_instructions=True
                    )

                    if choice not in ["y", "yes"]:
                        print(f"‚è≠Ô∏è  Skipping {file_info['path']}")
                        continue

                    # If user wants to keep parts of the file, we could merge here
                    if instructions:
                        print(f"üí° Note: {instructions}")
                        print("   (Manual merge may be needed)")

                file_path.write_text(content, encoding="utf-8")
                print(f"‚úì Created: {file_info['path']}")

            elif action == "modify":
                if not file_path.exists():
                    print(f"‚úó File does not exist: {file_info['path']}")
                    create_new = input(f"Create new file instead? (y/n): ").strip().lower()
                    if create_new != "y":
                        continue

                file_path.write_text(content, encoding="utf-8")
                print(f"‚úì Modified: {file_info['path']}")

        except Exception as e:
            print(f"‚úó Error applying change to {file_info['path']}: {e}")
            print(f"\nüìã Manual action required:")
            print(f"   1. Check file permissions for {file_info['path']}")
            print(f"   2. Ensure the parent directory exists")
            print(f"   3. Verify disk space availability")

            choice, _ = get_user_input("Continue with remaining files? (y/n)")
            if choice not in ["y", "yes"]:
                return False

    # Apply test files
    tests = code_result.get("tests", [])
    if tests:
        print(f"\nüß™ Creating {len(tests)} test file(s)...")

    for test_info in tests:
        test_path = target_dir / test_info["path"]
        content = test_info.get("content", "")

        try:
            test_path.parent.mkdir(parents=True, exist_ok=True)

            if test_path.exists():
                print(f"‚ö†Ô∏è  Test file already exists: {test_info['path']}")
                choice, _ = get_user_input(f"Overwrite {test_info['path']}? (y/n)")
                if choice not in ["y", "yes"]:
                    continue

            test_path.write_text(content, encoding="utf-8")
            print(f"‚úì Created test: {test_info['path']}")
        except Exception as e:
            print(f"‚úó Error creating test {test_info['path']}: {e}")
            choice, _ = get_user_input("Continue anyway? (y/n)")
            if choice not in ["y", "yes"]:
                return False

    print(f"\n‚úÖ Applied {len(files)} code file(s) and {len(tests)} test file(s)")
    return True


def run_tests(target_dir: Path, test_type: str = "all", auto_fix: bool = True, readme_content: str = "") -> TestResult:
    """
    Run tests and return results with optional auto-fixing using debugging agent.

    Args:
        target_dir: Project directory
        test_type: Type of tests to run
        auto_fix: If True, automatically try to fix failing tests
        readme_content: README.md content for debugging agent context

    Returns:
        TestResult with pass/fail status and details
    """
    print(f"\nüß™ Running {test_type} tests...")

    # Detect test framework
    test_commands = []

    # Python
    if (target_dir / "pytest.ini").exists() or any(target_dir.rglob("test_*.py")):
        test_commands.append("pytest -v")

    # Node.js
    package_json = target_dir / "package.json"
    if package_json.exists():
        try:
            with package_json.open() as f:
                pkg = json.load(f)
                if "test" in pkg.get("scripts", {}):
                    test_commands.append("npm test")
        except:
            pass

    if not test_commands:
        print("‚ö†Ô∏è  No test framework detected")
        return TestResult(
            passed=True,
            output="No tests to run",
            errors=[],
            test_count=0,
            failed_count=0
        )

    all_passed = True
    all_output = []
    all_errors = []

    for cmd in test_commands:
        retry_count = 0
        last_error = None
        stuck_count = 0  # Track if we're stuck with the same error

        while retry_count < MAX_AUTO_RETRY_ATTEMPTS:
            print(f"\n‚ñ∂ Running: {cmd}")
            try:
                result = subprocess.run(
                    cmd,
                    shell=True,
                    cwd=target_dir,
                    capture_output=True,
                    text=True,
                    timeout=300
                )

                all_output.append(result.stdout)

                if result.returncode == 0:
                    print(f"‚úì All tests passed")
                    break  # Tests passed, move to next command

                # Tests failed
                error_output = result.stderr or result.stdout
                all_errors.append(error_output)

                print(f"‚úó Tests failed")
                print(f"\nüìä Test Output:")
                print(error_output[:1000])  # Show first 1000 chars

                # Check if we're stuck with the same error
                if last_error and error_output[:500] == last_error[:500]:
                    stuck_count += 1
                    if stuck_count >= 3:
                        print(f"\n‚ö†Ô∏è  Same error seen {stuck_count} times - trying different approach...")
                else:
                    stuck_count = 0
                last_error = error_output

                if not auto_fix or retry_count >= MAX_AUTO_RETRY_ATTEMPTS - 1:
                    all_passed = False
                    break

                # Use debugging agent to analyze and fix the test failure
                print(f"\nüîÑ Launching debugging agent... (attempt {retry_count + 1}/{MAX_AUTO_RETRY_ATTEMPTS})")

                # Get file tree with contents for debugging agent
                file_tree = get_file_tree_with_contents(target_dir)

                # Build context with stuck detection info
                context_msg = f"running {test_type} tests"
                if stuck_count >= 3:
                    context_msg += f" (WARNING: Same error repeated {stuck_count} times - previous fixes didn't work, try a completely different approach)"

                # Analyze and get fix
                debug_fix = analyze_and_fix_test_failure(
                    error_log=error_output,
                    file_tree=file_tree,
                    target_dir=target_dir,
                    context=context_msg,
                    readme_content=readme_content
                )

                if debug_fix.needs_human:
                    print(f"\n‚ö†Ô∏è  Debugging agent: Human intervention needed")
                    print(f"\n{debug_fix.explanation}")
                    if debug_fix.human_instructions:
                        print(f"\nüìã Required action:")
                        print(f"  {debug_fix.human_instructions}")
                    all_passed = False
                    break

                # Apply the fix from debugging agent
                print(f"\nüí° Debugging agent fix ({debug_fix.confidence} confidence):")
                print(f"   {debug_fix.explanation}")

                # Apply file deletions
                for file_path in debug_fix.files_to_delete:
                    try:
                        (target_dir / file_path).unlink()
                        print(f"   ‚úì Deleted: {file_path}")
                    except Exception as e:
                        print(f"   ‚úó Failed to delete {file_path}: {e}")

                # Apply file creations
                for file_info in debug_fix.files_to_create:
                    try:
                        file_path = target_dir / file_info["path"]
                        file_path.parent.mkdir(parents=True, exist_ok=True)
                        file_path.write_text(file_info["content"], encoding="utf-8")
                        print(f"   ‚úì Created: {file_info['path']}")
                    except Exception as e:
                        print(f"   ‚úó Failed to create {file_info['path']}: {e}")

                # Apply file modifications
                for file_info in debug_fix.files_to_modify:
                    try:
                        file_path = target_dir / file_info["path"]
                        file_path.write_text(file_info["content"], encoding="utf-8")
                        print(f"   ‚úì Modified: {file_info['path']}")
                    except Exception as e:
                        print(f"   ‚úó Failed to modify {file_info['path']}: {e}")

                # Run commands
                for cmd in debug_fix.commands_to_run:
                    print(f"   ‚ñ∂ Running: {cmd}")
                    try:
                        result = subprocess.run(
                            cmd,
                            shell=True,
                            cwd=target_dir,
                            capture_output=True,
                            text=True,
                            timeout=60
                        )
                        if result.returncode == 0:
                            print(f"   ‚úì Success")
                        else:
                            print(f"   ‚ö†Ô∏è  Command failed: {result.stderr[:200]}")
                    except Exception as e:
                        print(f"   ‚úó Error running command: {e}")

                retry_count += 1

            except subprocess.TimeoutExpired:
                print(f"‚úó Tests timed out after 300 seconds")
                all_passed = False
                break
            except Exception as e:
                all_errors.append(str(e))
                print(f"‚úó Error running tests: {e}")
                all_passed = False
                break

    return TestResult(
        passed=all_passed,
        output="\n".join(all_output),
        errors=all_errors,
        test_count=0,  # Could parse from output
        failed_count=0  # Could parse from output
    )


def check_for_errors(target_dir: Path, env_info: dict[str, Any], auto_fix: bool = True) -> tuple[bool, list[str]]:
    """
    Check for syntax errors and linting issues with auto-fix.

    Returns:
        (all_fixed, remaining_errors) - True if all errors fixed, list of unfixed errors
    """
    print("\nüîç Checking for errors...")

    errors = []
    project_type = env_info.get("project_type")
    files_with_errors = []

    # Python syntax check
    if project_type == "python":
        for py_file in target_dir.rglob("*.py"):
            # Skip common directories
            if any(part.startswith(".") or part in ["venv", "env", "__pycache__"]
                   for part in py_file.parts):
                continue

            try:
                result = subprocess.run(
                    f"python -m py_compile {py_file}",
                    shell=True,
                    cwd=target_dir,
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                if result.returncode != 0:
                    error_msg = f"Syntax error in {py_file.name}: {result.stderr}"
                    errors.append(error_msg)
                    files_with_errors.append((py_file, result.stderr))
            except Exception as e:
                errors.append(f"Error checking {py_file.name}: {e}")

    # Node.js syntax check
    if project_type == "node":
        try:
            result = subprocess.run(
                "npm run lint 2>&1 || true",
                shell=True,
                cwd=target_dir,
                capture_output=True,
                text=True,
                timeout=60
            )
            if "error" in result.stdout.lower():
                errors.append(f"Linting errors found:\n{result.stdout}")
        except Exception as e:
            pass  # Linting is optional

    if not errors:
        print("‚úì No errors detected")
        return True, []

    print(f"‚úó Found {len(errors)} error(s)")
    for err in errors:
        print(f"  - {err[:200]}")  # Show first 200 chars

    if not auto_fix:
        return False, errors

    # Try to auto-fix errors
    print(f"\nüîÑ Attempting to auto-fix {len(files_with_errors)} file(s) with errors...")

    for file_path, error_msg in files_with_errors:
        print(f"\nüìù Fixing: {file_path.name}")

        try:
            # Read the file content
            original_content = file_path.read_text()

            # Get auto-fix suggestion
            fix_result = auto_fix_error(
                error_message=error_msg,
                context=f"syntax error in {file_path.name}",
                previous_attempt=original_content,
                retry_count=0
            )

            if fix_result.get("needs_human"):
                print(f"  ‚ö†Ô∏è  Cannot auto-fix {file_path.name}")
                print(f"  {fix_result.get('explanation', 'Manual fix required')}")
                continue

            # Apply the fix
            fixed_content = fix_result.get("solution")
            if fixed_content and fixed_content != original_content:
                file_path.write_text(fixed_content)
                print(f"  ‚úì Applied fix: {fix_result.get('explanation', 'Fixed syntax error')}")

                # Verify the fix worked
                result = subprocess.run(
                    f"python -m py_compile {file_path}",
                    shell=True,
                    cwd=target_dir,
                    capture_output=True,
                    text=True,
                    timeout=30
                )

                if result.returncode == 0:
                    print(f"  ‚úÖ Fix verified - no more errors in {file_path.name}")
                    errors.remove(f"Syntax error in {file_path.name}: {error_msg}")
                else:
                    print(f"  ‚ö†Ô∏è  Fix didn't resolve the error")
                    # Rollback
                    file_path.write_text(original_content)

        except Exception as e:
            print(f"  ‚úó Error during auto-fix: {e}")

    remaining_errors = [e for e in errors if e]  # Filter out removed errors

    if not remaining_errors:
        print(f"\n‚úÖ All errors fixed!")
        return True, []
    else:
        print(f"\n‚ö†Ô∏è  {len(remaining_errors)} error(s) remaining")
        return False, remaining_errors


def execute_task(
    task: dict[str, Any],
    task_index: int,
    context: ExecutionContext
) -> bool:
    """Execute a single task with testing and validation."""
    print("\n" + "=" * 80)
    print(f"üöÄ EXECUTING TASK {task_index + 1}: {task.get('title')}")
    print("=" * 80)

    # Generate code for the task
    code_result = generate_task_code(task, context, task_index)
    if not code_result:
        return False

    # Show what will be implemented
    print(f"\nüìã Implementation plan:")
    print(f"   {code_result.get('description', 'N/A')}")

    # Get user input with optional modifications
    choice, additional_instructions = get_user_input(
        "Proceed with implementation? (y/n/skip, or add modifications like 'y, but also add error logging')",
        target_dir=context.target_dir,
        context=f"Task {task_index + 1} ({task.get('title')}): Implementation approval"
    )

    if choice == "skip":
        print("‚è≠Ô∏è  Skipping this task")
        return True
    elif choice not in ["y", "yes"]:
        print("‚ùå Task cancelled")
        return False

    # If user added instructions, regenerate with modifications
    if additional_instructions:
        print(f"\nüí° Applying modifications: {additional_instructions}")
        modified_code = generate_task_code(
            {**task, "description": f"{task.get('description')}\n\nAdditional requirements: {additional_instructions}"},
            context,
            task_index
        )
        if modified_code:
            code_result = modified_code

    # Apply code changes
    if not apply_code_changes(code_result, context.target_dir):
        print("\n‚ùå Failed to apply code changes")
        choice, _ = get_user_input(
            "Retry? (y/n)",
            target_dir=context.target_dir,
            context=f"Task {task_index + 1}: Failed to apply code changes"
        )
        if choice in ["y", "yes"]:
            return execute_task(task, task_index, context)
        return False

    # Check for errors with auto-fix
    env_info = {"project_type": detect_project_type(context.target_dir)}
    all_fixed, remaining_errors = check_for_errors(context.target_dir, env_info, auto_fix=True)

    if not all_fixed:
        print(f"\n‚ùå {len(remaining_errors)} error(s) could not be auto-fixed")
        print("\n‚ö†Ô∏è  Human intervention required:")
        for i, error in enumerate(remaining_errors, 1):
            print(f"\n  {i}. {error}")

        print("\nüìã Please fix these errors manually, then choose an option:")
        choice, _ = get_user_input(
            "Options:\n  1. I've fixed the errors, continue\n  2. Retry (regenerate code)\n  3. Skip this task\n  4. Abort",
            target_dir=context.target_dir,
            context=f"Task {task_index + 1}: {len(remaining_errors)} syntax errors could not be auto-fixed"
        )

        if choice == "1":
            # Re-check errors
            all_fixed, remaining = check_for_errors(context.target_dir, env_info, auto_fix=False)
            if not all_fixed:
                print("‚ö†Ô∏è  Errors still present")
                return execute_task(task, task_index, context)
        elif choice == "2":
            return execute_task(task, task_index, context)
        elif choice == "3":
            print("‚è≠Ô∏è  Skipping task")
            return True
        else:
            return False

    # Run tests with auto-fix
    test_result = run_tests(context.target_dir, auto_fix=True, readme_content=context.readme_content)

    if not test_result.passed:
        print("\n‚ùå Tests failed after auto-fix attempts!")
        print("\nüìä Test Output:")
        print(test_result.output[:1500])  # Show first 1500 chars

        print("\n‚ö†Ô∏è  Human intervention required to fix failing tests")
        print("\nüìã Please review the test failures and fix the implementation")

        choice, _ = get_user_input(
            "Options:\n  1. I've fixed it, re-run tests\n  2. Retry task (regenerate)\n  3. Skip this task\n  4. Abort",
            target_dir=context.target_dir,
            context=f"Task {task_index + 1}: Tests failed after {MAX_AUTO_RETRY_ATTEMPTS} auto-fix attempts"
        )

        if choice == "1":
            # Re-run tests
            retry_result = run_tests(context.target_dir, auto_fix=False, readme_content=context.readme_content)
            if not retry_result.passed:
                print("‚ö†Ô∏è  Tests still failing")
                return execute_task(task, task_index, context)
            # Tests passed, continue
        elif choice == "2":
            return execute_task(task, task_index, context)
        elif choice == "3":
            print("‚è≠Ô∏è  Skipping task")
            return True
        else:
            return False

    # Integration test with previous tasks
    if context.completed_tasks:
        print(f"\nüîó Running integration tests with previous tasks...")
        integration_result = run_tests(context.target_dir, test_type="integration", auto_fix=True, readme_content=context.readme_content)

        if not integration_result.passed:
            print("\n‚ùå Integration tests failed!")
            print("‚ö†Ô∏è  This task may have broken existing functionality")

            print("\nüìã Options:")
            choice, _ = get_user_input(
                "  1. Try to fix automatically\n  2. I'll fix it manually\n  3. Rollback this task\n  4. Continue anyway (risky)",
                target_dir=context.target_dir,
                context=f"Task {task_index + 1}: Integration tests failed"
            )

            if choice == "1":
                # Already tried auto-fix in run_tests, so regenerate
                print("\nüîÑ Regenerating task with integration awareness...")
                return execute_task(task, task_index, context)
            elif choice == "2":
                print("\nüìã Please fix the integration issues, then:")
                retry_choice, _ = get_user_input(
                    "  1. I've fixed it, re-run integration tests\n  2. Abort",
                    target_dir=context.target_dir,
                    context=f"Task {task_index + 1}: User chose manual fix for integration issues"
                )
                if retry_choice == "1":
                    retry_result = run_tests(context.target_dir, test_type="integration", auto_fix=False, readme_content=context.readme_content)
                    if not retry_result.passed:
                        print("‚ö†Ô∏è  Integration tests still failing")
                        return False
                else:
                    return False
            elif choice == "3":
                print("‚è™ Rollback not yet implemented. Please revert changes manually.")
                return False
            elif choice == "4":
                print("‚ö†Ô∏è  Continuing with integration test failures (risky)")
                # Continue anyway
            else:
                return False

    print(f"\n‚úÖ Task {task_index + 1} completed successfully!")
    print(f"   ‚úì Code implemented")
    print(f"   ‚úì No syntax errors")
    print(f"   ‚úì All tests passing")
    if context.completed_tasks:
        print(f"   ‚úì Integration tests passing")

    context.completed_tasks.append(task.get('title', f'Task {task_index + 1}'))

    # Refresh file tree snapshot so next task knows what files exist
    print(f"\nüì∏ Refreshing file tree snapshot...")
    context.file_tree_snapshot = take_file_tree_snapshot(context.target_dir)

    return True


def detect_project_type(target_dir: Path) -> str:
    """Quick project type detection."""
    if (target_dir / "package.json").exists():
        return "node"
    if any((target_dir / f).exists() for f in ["requirements.txt", "pyproject.toml"]):
        return "python"
    return "unknown"


def start_execution(architecture_path: Path, tasks_path: Path, target_dir: Path) -> None:
    """Start the execution phase after user approval."""
    print("\n" + "=" * 80)
    print("üéØ STARTING EXECUTION PHASE")
    print("=" * 80)

    # Load architecture and tasks
    with architecture_path.open() as f:
        architecture = json.load(f)

    with tasks_path.open() as f:
        tasks_data = json.load(f)

    tasks = tasks_data.get("major_tasks", [])

    # Take file tree snapshot
    snapshot = take_file_tree_snapshot(target_dir)

    # Generate README.md for all agents to reference
    readme_content = generate_readme(architecture, target_dir, snapshot)

    # Detect environment
    env_info = detect_environment(snapshot, target_dir)

    # Setup environment if needed
    if env_info.get("needs_setup"):
        if not setup_environment(env_info, target_dir, architecture):
            print("\n‚ùå Environment setup failed. Please set up manually and try again.")
            return

    # Create execution context
    context = ExecutionContext(
        target_dir=target_dir,
        architecture=architecture,
        tasks=tasks_data,
        completed_tasks=[],
        file_tree_snapshot=snapshot,
        readme_content=readme_content
    )

    # Execute tasks one by one
    print(f"\nüìã Total tasks to execute: {len(tasks)}")

    for i, task in enumerate(tasks):
        if not execute_task(task, i, context):
            print(f"\n‚ùå Execution stopped at task {i + 1}")
            return

    print("\n" + "=" * 80)
    print("üéâ ALL TASKS COMPLETED SUCCESSFULLY!")
    print("=" * 80)
    print(f"\nCompleted tasks:")
    for i, task_title in enumerate(context.completed_tasks, 1):
        print(f"  {i}. ‚úì {task_title}")
